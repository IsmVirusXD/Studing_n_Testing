{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93dd6390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ced78ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|   Igor|  23|        10| 30000|\n",
      "|  Vitor|  16|         8| 25000|\n",
      "|Marlene|  50|        20| 20000|\n",
      "|  Almir|  60|        15| 18000|\n",
      "| Letica|  21|        30|  5000|\n",
      "| Carpio|null|      null| 30000|\n",
      "|   null|  23|        10| 38000|\n",
      "|   null|  36|      null|  null|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('test2.csv',header=True,inferSchema=True).show()\n",
    "df_pyspark = spark.read.csv('test2.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d70b9f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|   Igor| 23|        10| 30000|\n",
      "|  Vitor| 16|         8| 25000|\n",
      "|Marlene| 50|        20| 20000|\n",
      "|  Almir| 60|        15| 18000|\n",
      "| Letica| 21|        30|  5000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###any == how -> Drop if it has Any Null\n",
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95765229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|   Igor|  23|        10| 30000|\n",
      "|  Vitor|  16|         8| 25000|\n",
      "|Marlene|  50|        20| 20000|\n",
      "|  Almir|  60|        15| 18000|\n",
      "| Letica|  21|        30|  5000|\n",
      "| Carpio|null|      null| 30000|\n",
      "|   null|  23|        10| 38000|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###Threshold => Dropa uma Linha que tenha no minimo N nÃ£o null\n",
    "df_pyspark.na.drop(how='all',thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "371b9673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|   Igor| 23|        10| 30000|\n",
      "|  Vitor| 16|         8| 25000|\n",
      "|Marlene| 50|        20| 20000|\n",
      "|  Almir| 60|        15| 18000|\n",
      "| Letica| 21|        30|  5000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###Subset -> Drop only in one especific colum\n",
    "df_pyspark.na.drop(how='any',subset=[\"Experience\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6bc882d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+----------+------+\n",
      "|         Name| Age|Experience|Salary|\n",
      "+-------------+----+----------+------+\n",
      "|         Igor|  23|        10| 30000|\n",
      "|        Vitor|  16|         8| 25000|\n",
      "|      Marlene|  50|        20| 20000|\n",
      "|        Almir|  60|        15| 18000|\n",
      "|       Letica|  21|        30|  5000|\n",
      "|       Carpio|null|      null| 30000|\n",
      "|Missing Value|  23|        10| 38000|\n",
      "|Missing Value|  36|      null|  null|\n",
      "+-------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Filling the Missing Value -> Vai substituir os valores faltando pelo que vc escreveu\n",
    "df_pyspark.na.fill('Missing Value',['Name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ef7ac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=['Age','Experience','Salary'],\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['Age','Experience','Salary']]\n",
    ").setStrategy(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7d918a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "|   Name| Age|Experience|Salary|Age_imputed|Experience_imputed|Salary_imputed|\n",
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "|   Igor|  23|        10| 30000|         23|                10|         30000|\n",
      "|  Vitor|  16|         8| 25000|         16|                 8|         25000|\n",
      "|Marlene|  50|        20| 20000|         50|                20|         20000|\n",
      "|  Almir|  60|        15| 18000|         60|                15|         18000|\n",
      "| Letica|  21|        30|  5000|         21|                30|          5000|\n",
      "| Carpio|null|      null| 30000|         32|                15|         30000|\n",
      "|   null|  23|        10| 38000|         23|                10|         38000|\n",
      "|   null|  36|      null|  null|         36|                15|         23714|\n",
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Add imputation cols to df\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
